{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"C:/Users/yuvan/OneDrive/Documents/Monash DeepNeuron/Artistic Articulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5405dd2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import config\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from discriminatorGAN import Discriminator\n",
    "from generatorGAN import Generator\n",
    "from datasetGAN import CustomDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import save_checkpoint, load_checkpoint, save_results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_discriminator(discriminator, generator, real_img, disc_optimizer, batch_size, latent_size, device):\n",
    "    # Reset the gradients for the optimizer\n",
    "    disc_optimizer.zero_grad()\n",
    "\n",
    "    # Train on the real images\n",
    "    real_predictions = discriminator(real_img)\n",
    "    # real_targets = torch.zeros(real_pokemon.size(0), 1, device=device) # All of these are real, so the target is 0.\n",
    "    real_targets = torch.ones(real_img.size(0), 1, device=device) # * (0.1 - 0) + 0  # Add some noisy labels to make the discriminator think harder.\n",
    "    real_loss = F.binary_cross_entropy(real_predictions, real_targets)  # Can do binary loss function because it is a binary classifier\n",
    "    real_score = torch.mean(real_predictions).item()  # How well does the discriminator classify the real pokemon? (Higher score is better for the discriminator)\n",
    "\n",
    "    # Make some latent tensors to seed the generator\n",
    "    latent_batch = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "\n",
    "    # Get some fake images\n",
    "    fake_imgs = generator(latent_batch)\n",
    "\n",
    "    # Train on the generator's current efforts to trick the discriminator\n",
    "    gen_predictions = discriminator(fake_imgs)\n",
    "    # gen_targets = torch.ones(fake_imgs.size(0), 1, device=device)\n",
    "    gen_targets = torch.zeros(fake_imgs.size(0), 1, device=device) # * (1 - 0.9) + 0.9  # Add some noisy labels to make the discriminator think harder.\n",
    "    gen_loss = F.binary_cross_entropy(gen_predictions, gen_targets)\n",
    "    gen_score = torch.mean(gen_predictions).item()  # How well did the discriminator classify the fake pokemon? (Lower score is better for the discriminator)\n",
    "\n",
    "    # Update the discriminator weights\n",
    "    total_loss = real_loss + gen_loss\n",
    "    total_loss.backward()\n",
    "    disc_optimizer.step()\n",
    "\n",
    "    return total_loss.item(), real_score, gen_score\n",
    "\n",
    "\n",
    "def train_generator(discriminator, generator, gen_optimizer, batch_size, latent_size, device):\n",
    "    # Clear the generator gradients\n",
    "    gen_optimizer.zero_grad()\n",
    "\n",
    "    # Generate some fake pokemon\n",
    "    latent_batch = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_img = generator(latent_batch)\n",
    "\n",
    "    # Test against the discriminator\n",
    "    disc_predictions = discriminator(fake_img)\n",
    "    targets = torch.ones(fake_img.size(0), 1, device=device)  # We want the discriminator to think these images are real.\n",
    "    loss = F.binary_cross_entropy(disc_predictions, targets)  # How well did the generator do? (How much did the discriminator believe the generator?)\n",
    "\n",
    "    # Update the generator based on how well it fooled the discriminator\n",
    "    loss.backward()\n",
    "    gen_optimizer.step()\n",
    "\n",
    "    # Return generator loss\n",
    "    return loss.item()\n",
    "\n",
    "def train(discriminator, generator, data, epochs, disc_optimizer, gen_optimizer, start_idx=1):\n",
    "    # Empty the GPU cache to save some memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # For results viewing\n",
    "    fixed_latent_batch = torch.randn(config.BATCH_SIZE, config.LATENT_SIZE, 1, 1, device=config.DEVICE)\n",
    "\n",
    "    # Track losses and scores\n",
    "    disc_losses = []\n",
    "    disc_scores = []\n",
    "    gen_losses = []\n",
    "    gen_scores = []\n",
    "\n",
    "    # Run the loop\n",
    "    for epoch in range(epochs):\n",
    "        # Go through each image\n",
    "        for real_img in tqdm(data):\n",
    "            real_img = real_img.to(config.DEVICE) # make sure to send data to gpu\n",
    "            # Train the discriminator\n",
    "            disc_loss, real_score, gen_score = train_discriminator(discriminator, generator, real_img, disc_optimizer, batch_size=config.BATCH_SIZE, latent_size=config.LATENT_SIZE, device=config.DEVICE)\n",
    "\n",
    "            # Train the generator\n",
    "            gen_loss = train_generator(discriminator, generator, gen_optimizer, batch_size=config.BATCH_SIZE, latent_size=config.LATENT_SIZE, device=config.DEVICE)\n",
    "\n",
    "        # Collect results\n",
    "        disc_losses.append(disc_loss)\n",
    "        disc_scores.append(real_score)\n",
    "        gen_losses.append(gen_loss)\n",
    "        gen_scores.append(gen_score)\n",
    "\n",
    "        # Print the losses and scores\n",
    "        print(\"Epoch [{}/{}], gen_loss: {:.4f}, disc_loss: {:.4f}, real_score: {:.4f}, gen_score: {:.4f}\".format(\n",
    "            epoch + start_idx, epochs, gen_loss, disc_loss, real_score, gen_score))\n",
    "\n",
    "        # Save the images and show the progress\n",
    "        save_results(generator, epoch + start_idx, fixed_latent_batch, show=False)\n",
    "\n",
    "        if config.SAVE_MODEL and epoch % 10 == 0:\n",
    "            save_checkpoint(generator, gen_optimizer, filename=config.CHECKPOINT_GEN_Pokemon) # human faces\n",
    "            save_checkpoint(discriminator, disc_optimizer, filename=config.CHECKPOINT_DISC_Pokemon)\n",
    "\n",
    "    # Return stats\n",
    "    return disc_losses, disc_scores, gen_losses, gen_scores\n",
    "\n",
    "def main():\n",
    "    discriminator = Discriminator().to(config.DEVICE)\n",
    "    generator = Generator(config.LATENT_SIZE).to(config.DEVICE)\n",
    "\n",
    "    # Create the optimizers\n",
    "    disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.9))\n",
    "    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=config.LEARNING_RATE, betas=(0.5, 0.9))\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN_Pokemon,\n",
    "            generator,\n",
    "            gen_optimizer,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_DISC_Pokemon,\n",
    "            discriminator,\n",
    "            disc_optimizer,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    dset1 = CustomDataset(\"data/train/logos\", transform=config.transforms1)\n",
    "    #dset2 = CustomDataset(\"data/train/logos\", transform=config.transforms2)\n",
    "    #dsetlist = [dset1, dset2]\n",
    "    #dataset = torch.utils.data.ConcatDataset(dsetlist)\n",
    "    dataset = dset1\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    results = train(discriminator, generator, loader, config.NUM_EPOCHS, disc_optimizer, gen_optimizer)\n",
    "\n",
    "    disc_losses, disc_scores, gen_losses, gen_scores = results\n",
    "\n",
    "    plt.plot(disc_losses, '-')\n",
    "    plt.plot(gen_losses, '-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Discriminator', 'Generator'])\n",
    "    plt.title('Losses')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c7857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
